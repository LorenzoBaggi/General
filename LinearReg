# -*- coding: utf-8 -*-
"""
Created on Sun Jun  5 10:55:18 2022

@author: lawre
"""

import numpy as np
import pandas as pd
import quandl
spy_table = quandl.get('LSE/SPY5')
amzn_table = quandl.get('WIKI/AMZN')

spy = spy_table.loc['2016',['Last Close']]
amzn = amzn_table.loc['2016',['Close']]
spy_log = np.log(spy['Last Close']).diff().dropna()
amzn_log = np.log(amzn.Close).diff().dropna()

df = pd.concat([spy_log,amzn_log],axis = 1).dropna()
df.columns = ['spy','amzn']
df.tail()

import matplotlib.pyplot as plt
plt.figure(figsize = (15,10))
plt.scatter(df.spy,df.amzn)
plt.xlabel('spx_return')
plt.ylabel('amzn_return') 

import statsmodels.formula.api as sm
#It's natural that we want to model the relation between these two 
#rates of return. Intuitively we use a straight line to model it, this is called 
#Linear Regression. In order to find the best straight line, it's natural to
#think that the vertical distances between the points of the data set and 
#the fitted line should be minimized. Those vertical distances are called residual.
#Our objective is to make the sum of squared residuals as small as possible. 
#This method is called ordinary least square, or OLS method

#n python, we have a very power package for mathematical models, which is named 'statsmodels'.

model = sm.ols(formula = 'amzn~spy',data = df).fit()
print(model.summary())  

print('pamameters: ',model.params)
print('residual: ', model.resid.tail())
print('fitted values: ',model.predict()[-6:])

plt.figure(figsize = (15,10))
plt.scatter(df.spy,df.amzn)
plt.xlabel('spx_return')
plt.ylabel('amzn_return')
plt.plot(df.spy,model.predict(),color = 'red')
plt.show()
